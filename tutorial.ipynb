{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handwritten digit recogniser\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.utils import np_utils\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "from keras.models import model_from_yaml\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is used to visualise the images from the data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtIAAAHVCAYAAADchxyPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1843c96400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#load (download if needed) the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# plot 4 images as gray scale\n",
    "fig = plt.figure()\n",
    "plt.subplot(221)\n",
    "plt.imshow(X_train[0], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(222)\n",
    "plt.imshow(X_train[1], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(223)\n",
    "plt.imshow(X_train[2], cmap=plt.get_cmap('gray'))\n",
    "plt.subplot(224)\n",
    "plt.imshow(X_train[3], cmap=plt.get_cmap('gray'))\n",
    "\n",
    "fig.set_size_inches(16,8)\n",
    "\n",
    "\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defining some useful functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier():\n",
    "    \"\"\"\n",
    "    Creates an model for MNIST image classification with input dimension 784\n",
    "    returns: Keras model\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_pixels,input_dim=num_pixels,kernel_initializer='normal',activation='relu'))\n",
    "    model.add(Dense(int(num_pixels/2),activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(int(num_pixels/2),activation='relu'))\n",
    "    model.add(Dense(int(num_pixels/2),activation='relu'))\n",
    "    \n",
    "    model.add(Dense(num_classes,activation='softmax'))\n",
    "    \n",
    "    return model\n",
    "\n",
    "def writeResultsToFile(events,filePath):\n",
    "    \"\"\"\n",
    "    Writes events to pickle file. Ideally dump few objects where the objects could be any data structures\n",
    "    containing other objects\n",
    "    :param events:\n",
    "    :param filePath:\n",
    "    \"\"\"\n",
    "    print(\"Writing...\")\n",
    "    if filePath[-3:]!=\"pkl\":\n",
    "        filePath = filePath+\".pkl\"\n",
    "\n",
    "    with open(filePath, \"wb\") as output:\n",
    "\n",
    "        pickle.dump(events, output, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "def loadResultsFromFile(filePath,python2 = False):\n",
    "    \"\"\"\n",
    "    Loads objects from pickle file\n",
    "    :param filePath:\n",
    "    :return: values in pickle file\n",
    "    \"\"\"\n",
    "    load = []\n",
    "    print(\"Loading...\")\n",
    "    with open(filePath, \"rb\") as file:\n",
    "        hasNext = True\n",
    "        if python2:\n",
    "\n",
    "            load.append(pickle.load(file))\n",
    "        else:\n",
    "            load.append(pickle.load(file, encoding='latin1'))\n",
    "        while hasNext:\n",
    "            try:\n",
    "                if python2:\n",
    "                    load.append(pickle.load(file))\n",
    "                else:\n",
    "                    load.append(pickle.load(file, encoding='latin1'))\n",
    "            except:\n",
    "                hasNext = False\n",
    "\n",
    "    if len(load) == 1:\n",
    "        return load[0]\n",
    "    else:\n",
    "        return load\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 18s 307us/step - loss: 0.2395 - acc: 0.9286 - val_loss: 0.1120 - val_acc: 0.9645\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 17s 283us/step - loss: 0.0877 - acc: 0.9728 - val_loss: 0.0800 - val_acc: 0.9737\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 18s 303us/step - loss: 0.0583 - acc: 0.9813 - val_loss: 0.0742 - val_acc: 0.9769\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 18s 303us/step - loss: 0.0470 - acc: 0.9855 - val_loss: 0.0717 - val_acc: 0.9783\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 17s 292us/step - loss: 0.0357 - acc: 0.9891 - val_loss: 0.0738 - val_acc: 0.9789\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 19s 316us/step - loss: 0.0275 - acc: 0.9915 - val_loss: 0.0702 - val_acc: 0.9822\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 20s 328us/step - loss: 0.0273 - acc: 0.9915 - val_loss: 0.0951 - val_acc: 0.9758\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 16s 267us/step - loss: 0.0259 - acc: 0.9917 - val_loss: 0.0797 - val_acc: 0.9789\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 16s 270us/step - loss: 0.0224 - acc: 0.9929 - val_loss: 0.0631 - val_acc: 0.9840\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 17s 275us/step - loss: 0.0175 - acc: 0.9946 - val_loss: 0.0743 - val_acc: 0.9813\n",
      "Writing...\n"
     ]
    }
   ],
   "source": [
    "# flatten 28*28 images to a 784 vector for each image\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "num_pixels = x_train.shape[1] * x_train.shape[2]\n",
    "x_train = x_train.reshape(x_train.shape[0], num_pixels).astype('float32')\n",
    "x_test = x_test.reshape(x_test.shape[0], num_pixels).astype('float32')\n",
    "\n",
    "# one hot encode outputs\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "\n",
    "# Scale Data \n",
    "scaler = preprocessing.MinMaxScaler(feature_range=(0,1))\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)\n",
    "\n",
    "scaler.fit(x_test)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "\n",
    "# Create and compile model\n",
    "\n",
    "model = classifier()\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,validation_data=(x_test, y_test), epochs=10, batch_size=200,verbose = 1)\n",
    "\n",
    "\n",
    "yaml_string = model.to_yaml()\n",
    "writeResultsToFile(yaml_string,\"model_yaml.pkl\")\n",
    "model.save_weights('neural_net_weights')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n"
     ]
    }
   ],
   "source": [
    "model_loaded = loadResultsFromFile('model_yaml.pkl')\n",
    "model_loaded = model_from_yaml(model_loaded)\n",
    "model_loaded.load_weights('neural_net_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing loaded model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# y_train = np_utils.to_categorical(y_train)\n",
    "# y_test = np_utils.to_categorical(y_test)\n",
    "num_classes = y_test.shape[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking : expected dense_13_input to have shape (None, 784) but got array with shape (784, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-b138c8806840>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ishankhurana/anaconda3/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1007\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ishankhurana/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1763\u001b[0m         x = _standardize_input_data(x, self._feed_input_names,\n\u001b[1;32m   1764\u001b[0m                                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1765\u001b[0;31m                                     check_batch_axis=False)\n\u001b[0m\u001b[1;32m   1766\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1767\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ishankhurana/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    151\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking : expected dense_13_input to have shape (None, 784) but got array with shape (784, 1)"
     ]
    }
   ],
   "source": [
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "example = x_train[0]\n",
    "example = example.reshape(1,784)\n",
    "solution  = model.predict(example)\n",
    "\n",
    "print(np.argmax(solution))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# model = model_from_yaml(yaml_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n"
     ]
    }
   ],
   "source": [
    "def loadResultsFromFile(filePath,python2 = False):\n",
    "    \"\"\"\n",
    "    Loads objects from pickle file\n",
    "    :param filePath:\n",
    "    :return: values in pickle file\n",
    "    \"\"\"\n",
    "    load = []\n",
    "    print(\"Loading...\")\n",
    "    with open(filePath, \"rb\") as file:\n",
    "        hasNext = True\n",
    "        if python2:\n",
    "\n",
    "            load.append(pickle.load(file))\n",
    "        else:\n",
    "            load.append(pickle.load(file, encoding='latin1'))\n",
    "        while hasNext:\n",
    "            try:\n",
    "                if python2:\n",
    "                    load.append(pickle.load(file))\n",
    "                else:\n",
    "                    load.append(pickle.load(file, encoding='latin1'))\n",
    "            except:\n",
    "                hasNext = False\n",
    "\n",
    "    if len(load) == 1:\n",
    "        return load[0]\n",
    "    else:\n",
    "        return load\n",
    "    \n",
    "\n",
    "model_loaded = loadResultsFromFile('model_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading...\n"
     ]
    }
   ],
   "source": [
    "model_loaded = loadResultsFromFile('model_1.pkl')\n",
    "model_loaded = model_from_yaml(model_loaded)\n",
    "model_loaded.load_weights('neural_net_work')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "example = x_train[0]\n",
    "example = example.reshape(1,784)\n",
    "\n",
    "print(np.argmax(model_loaded.predict(example)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
